
## TODO
1. 深入了解**SeparableConv**、**DepthwiseConv**、**BatchNormalization**的计算公式和操作。
2. 如何对模型进行集成。找一些kaggle的例子。

## 第四章 机器学习基础
现在`dropout`操作用的很多，但是我之前全部都理解错了，在训练的时候加上`dropout`，它会屏蔽一部分的节点，在测试的时候不会屏蔽，但是输出的时候要乘以`dropout`的比率系数。

## 第五章 深度学习用于计算机视觉
本章对我来说收益最大的是卷积神经网络的可视化。主要有三种可视化，1. 是把每一层的激活层的输出可视化，它表示经过神经网络之后，每一层的输出结果是什么。从这个可以看出来，前面的层主要是一些边缘探测器，它几乎保留了原始图像中的所有信息，随着层数的增加，提取的信息越来越抽象，越来越难以直观的理解，还有一种现象就是随着层数增加，越来越多的输出是空白的，也就是这些过滤器没有找到图像中符合的编码信息。2. 是可视化过滤器，我最开始的想法是，过滤器不就是每个卷积网络的kernel这些吗？这些不就是过滤器吗？但是这个有点过于底层，其实我们最想知道的是某一层的整体的过滤效果，而不是单个的，单个的其实很难看出什么东西。所以这里用了很巧妙的一种方法就是，我构建随机图像，然后经过过滤器，那么，当我的输入和过滤器的模式一样的时候，这样的响应是最大的，有点像模版匹配。可以利用梯度上升法，将这一层的输出相对于输入求导，然后沿着输出值最大的方向改变输入的值，使得最后的输出最大，最后得到的输入图像就是过滤器了。这部分能够看出的信息其实和第一部分是差不多的。3. 是类激活的热力图，也就是每一个像素，它预测是属于哪一种类，预测的概率是多少，这样可以看看哪些像素比较重要。该方法基于 ***Grad-CAM: visual explanations from deep networks via gradient-based localization*** 论文。主要的思路如下，卷积层的输出其实代表的就是原始图像中剩余的信息，剩余的信息就是比较重要的信息，重要性就体现在了特征图的值上，但是输出的通道很多，有512层，有128层，怎么把这些层合并成一层，然后映射到输入图像上呢。不同的层对于最后的预测结果的影响是不同的，那么按照这个影响的权重进行相加就是合理的，因此可以求出特征图中的每一层相对于最后预测的类别的梯度代表着它的重要性系数，然后把卷积层的输出进行叠加就可以了。

在`keras`中，其实并不是模型为导向的，平时写模型，都是基于模型的思维在计算梯度、loss函数之类的，其实还可以基于`function`来计算，通过指定输入和输出的`Tensor`，可以生成`function`，然后和模型中用到的计算思路一样，每一次输入输入，得到输出。

## 第七章 高级的深度学习最佳实践
### 多模态模型构建
在前面几章的，构建模型都是用的`Sequential`模块，把一层一层的`layer`叠加在一起，只有一个输入一个输出，但是实际上在构建模型的时候，常常会遇到多输入或者多输出的情况，还有一些中间比较复杂的模块，这个时候就要用到`Model`模块了。在`keras`中有两种模块，分别是`Mode`和`Network`，`Network`只包括网络的结构，而`Model`在这个基础上还包括了训练的部分。之前写代码我只关心整个的网络结构，训练的部分都是手工进行，这样写代码就有点不优雅。之前之所以手工写，是因为官网的例子都是很简单的例子，多输入多输出的情况都很少，在多种输入和输出的情况下，应该怎么计算loss，怎么把不同的输入和不同的输出进行对应，这点我没有想明白。其实很简单，就是每一层都有名字，输入输出层也有自己的名字，在输入对应数据的时候，通过`dict`类型进行赋值或对应。如下代码所示，同理可得，也可以分别计算loss。

``` python
brancha = Input(shape=(300,300,3), name="brancha")
branchb = Input(shape=(300,300,3), name="branchb")

xa = Dense(32,3,activation="relu")(brancha)
xb = Dense(32,3,activation="relu")(branchb)

x = Concatenate()([xa, xb])

outputa = Dense(32,3,activation="softmax", name="outputa")(x)
outputb = Dense(32,3,activation="softmax", name="outputb")(x)

model = Model(inputs=[brancha, branchb], outpus = [outputa, outputb])

model.fit({"brancha":inputa,"branchb":inputb},
          {"outputa":outputa, "outputb":outputb},
          epochs=10, batch_size=10)
```

另一方面，我从最开始写模型是通过继承`Model`这个类，然后分别写好每一层，设置好参数，然后在`__call__`中调用这些定义好的层。这在面向对象的逻辑中是成立的，但是在这里实际操作的时候就不好操作，首先，不同模型之间的复用就会变得很麻烦，会行程模型一层嵌套一层，最后把整个代码弄的很复杂，同时这么写的话，就需要考虑自己推断从输入的shape到输出的shape，同时很多的函数也没办法调用，比如`summary()`等。所以现在我转换了策略，我依然继承`Model`，但是我在初始化的时候，通过构建输入与输出，然后把构建的`inputs`和`outputs`传入到父类中去构建出整个的模型。这样，既能根据不同的模型加入不同的参数，也能用现成的keras模型。

### 有用的结构和卷积
在做卷积操作的时候，其实有两步：1.是在同一层中，用同一个卷积核遍历该特征层；2.在不同的特征层中进行运算求和。前者叫空间特征，后者叫通道特征。**1x1卷积**就是计算的通道特征，**SeparableConv**和**DepthwiseConv**就是计算的空间的特征。

上述提到的非线性的模块包括了Inception模块和residual模块。residual模块之所以效果这么好一方面是因为加了shortcut模块，因为在进行卷积操作的时候，其实是把一些特征丢失的，很多丢失的信号是没办法复原的。比如把音频中的低频信号去掉，就没办法恢复了。shortcut模块可以把前面的信息再加回去。另一方面这个通道也能够在反向传播的时候发挥作用，减缓出现梯度消失的情况。

书中还提到一种情况，虽然有两个输入，但是两个输入的属性一样，比如要评估两个句子之间的语意相似度，因为两个输入的句子是可以互换的，他们的相似度是对称关系，不能说用两个网络分别去处理，因为他们是把句子映射到同一特征空间，所以会用同一个模型去分别走两个句子，然后得到的结果再去评估句子的相似度。处理两个摄像头的特征也是同理。只不过这部分在实际的应用中我还没有见到。

### 回调函数
回调函数的作用其实很多方面，有一点我没有想到，就是可以设置提前训练结束还有根据loss函数，梯度等动态调整learning rate等。

### 进一步提升模型的性能
#### 批处理
对输入数据进行标准化，可以剔除一些无关紧要的特征的影响，专注于整个图像的分布不同进行推理得到结果。从每一层的特征图出发，其实和从输入层出发一样，也需要进行标准化，可以怎么标准化，之前在ResNet中用了一组RGB的数字是统计了ImageNet的图片得到的均值和方差值。所以在`BatchNormalization`的计算中，不断更新均值等，在训练过程中保存已读取的每批的数据均值和方差的指数移动平均值，它有利于梯度传播，因此运行更深的网络。

#### 超参数优化
超参数优化需要注意的点就是，我们要用验证集来调整，不要基于测试集，不然就没啥用了，容易出现过拟合。

文中也推荐了超参数优化的库：**Hyperopt**，它的内部使用Parzen估计器来预测哪组超参数更好。还有一种和keras结合的**Hyperas**库，也可以尝试一下。

#### 模型集成
模型之所以集成，是因为不同的模型从不同的侧面反应出事物的不同方面。就像盲人摸象，一个模型摸到一个地方，另一个模型摸到另一个地方，然后集成，最后勾勒出整个大象的形状。但是也不能都摸到大象的鼻子，这样的集成是没有用的，必须要保证模型的多样性，不同的模型，偏差向不同的地方。

**但是，怎么去评估各个模型的得分，效果呢？**，去kaggle上多找一些例子看看。

## 第八章


## 第九章 总结
总的来说，现阶段的深度学习其实是局部泛化，也就是说我喂进去什么数据，只能基于这个进行简单的泛化，比如输入识别人的数据，就不能识别其他动物。而人类的意识其实是极端泛化，我们不需要反复的看很多数据，能够通过**抽象**和**推理**进行更宽泛的泛化，所以，如果要实现人类一样的智慧，现在的这种方法肯定是不行的。

想要实现模型的复用其实很难，模型结构可以复用，可是模型参数怎么办，模型参数是根据不同的任务训练的，是没办法复用的，所以现阶段的这种形式肯定是初级阶段的。