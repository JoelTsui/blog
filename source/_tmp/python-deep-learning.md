
## TODO
1. 深入了解**SeparableConv**、**DepthwiseConv**、**BatchNormalization**的计算公式和操作。
2. 如何对模型进行集成。找一些kaggle的例子。



## 第七章 高级的深度学习最佳实践
### 多模态模型构建
在前面几章的，构建模型都是用的`Sequential`模块，把一层一层的`layer`叠加在一起，只有一个输入一个输出，但是实际上在构建模型的时候，常常会遇到多输入或者多输出的情况，还有一些中间比较复杂的模块，这个时候就要用到`Model`模块了。在`keras`中有两种模块，分别是`Mode`和`Network`，`Network`只包括网络的结构，而`Model`在这个基础上还包括了训练的部分。之前写代码我只关心整个的网络结构，训练的部分都是手工进行，这样写代码就有点不优雅。之前之所以手工写，是因为官网的例子都是很简单的例子，多输入多输出的情况都很少，在多种输入和输出的情况下，应该怎么计算loss，怎么把不同的输入和不同的输出进行对应，这点我没有想明白。其实很简单，就是每一层都有名字，输入输出层也有自己的名字，在输入对应数据的时候，通过`dict`类型进行赋值或对应。如下代码所示，同理可得，也可以分别计算loss。

``` python
brancha = Input(shape=(300,300,3), name="brancha")
branchb = Input(shape=(300,300,3), name="branchb")

xa = Dense(32,3,activation="relu")(brancha)
xb = Dense(32,3,activation="relu")(branchb)

x = Concatenate()([xa, xb])

outputa = Dense(32,3,activation="softmax", name="outputa")(x)
outputb = Dense(32,3,activation="softmax", name="outputb")(x)

model = Model(inputs=[brancha, branchb], outpus = [outputa, outputb])

model.fit({"brancha":inputa,"branchb":inputb},
          {"outputa":outputa, "outputb":outputb},
          epochs=10, batch_size=10)
```

另一方面，我从最开始写模型是通过继承`Model`这个类，然后分别写好每一层，设置好参数，然后在`__call__`中调用这些定义好的层。这在面向对象的逻辑中是成立的，但是在这里实际操作的时候就不好操作，首先，不同模型之间的复用就会变得很麻烦，会行程模型一层嵌套一层，最后把整个代码弄的很复杂，同时这么写的话，就需要考虑自己推断从输入的shape到输出的shape，同时很多的函数也没办法调用，比如`summary()`等。所以现在我转换了策略，我依然继承`Model`，但是我在初始化的时候，通过构建输入与输出，然后把构建的`inputs`和`outputs`传入到父类中去构建出整个的模型。这样，既能根据不同的模型加入不同的参数，也能用现成的keras模型。

### 有用的结构和卷积
在做卷积操作的时候，其实有两步：1.是在同一层中，用同一个卷积核遍历该特征层；2.在不同的特征层中进行运算求和。前者叫空间特征，后者叫通道特征。**1x1卷积**就是计算的通道特征，**SeparableConv**和**DepthwiseConv**就是计算的空间的特征。

上述提到的非线性的模块包括了Inception模块和residual模块。residual模块之所以效果这么好一方面是因为加了shortcut模块，因为在进行卷积操作的时候，其实是把一些特征丢失的，很多丢失的信号是没办法复原的。比如把音频中的低频信号去掉，就没办法恢复了。shortcut模块可以把前面的信息再加回去。另一方面这个通道也能够在反向传播的时候发挥作用，减缓出现梯度消失的情况。

书中还提到一种情况，虽然有两个输入，但是两个输入的属性一样，比如要评估两个句子之间的语意相似度，因为两个输入的句子是可以互换的，他们的相似度是对称关系，不能说用两个网络分别去处理，因为他们是把句子映射到同一特征空间，所以会用同一个模型去分别走两个句子，然后得到的结果再去评估句子的相似度。处理两个摄像头的特征也是同理。只不过这部分在实际的应用中我还没有见到。

### 回调函数
回调函数的作用其实很多方面，有一点我没有想到，就是可以设置提前训练结束还有根据loss函数，梯度等动态调整learning rate等。

### 进一步提升模型的性能
#### 批处理
对输入数据进行标准化，可以剔除一些无关紧要的特征的影响，专注于整个图像的分布不同进行推理得到结果。从每一层的特征图出发，其实和从输入层出发一样，也需要进行标准化，可以怎么标准化，之前在ResNet中用了一组RGB的数字是统计了ImageNet的图片得到的均值和方差值。所以在`BatchNormalization`的计算中，不断更新均值等，在训练过程中保存已读取的每批的数据均值和方差的指数移动平均值，它有利于梯度传播，因此运行更深的网络。

#### 超参数优化
超参数优化需要注意的点就是，我们要用验证集来调整，不要基于测试集，不然就没啥用了，容易出现过拟合。

文中也推荐了超参数优化的库：**Hyperopt**，它的内部使用Parzen估计器来预测哪组超参数更好。还有一种和keras结合的**Hyperas**库，也可以尝试一下。

#### 模型集成
模型之所以集成，是因为不同的模型从不同的侧面反应出事物的不同方面。就像盲人摸象，一个模型摸到一个地方，另一个模型摸到另一个地方，然后集成，最后勾勒出整个大象的形状。但是也不能都摸到大象的鼻子，这样的集成是没有用的，必须要保证模型的多样性，不同的模型，偏差向不同的地方。

**但是，怎么去评估各个模型的得分，效果呢？**，去kaggle上多找一些例子看看。


## 第九章 总结
