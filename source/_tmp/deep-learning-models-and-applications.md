---
title: 《深度学习模型及应用详解》读书笔记 
date: 
mathjax: true
---

## TODO
1. P57页中negative sample中的loss函数计算公式是怎么得到的。
2. 

## 第二章 深度学习开源框架
深度学习的变化真的很快，在2015年的时候，tensoflow刚出来，theano还是深度学习框架的霸主，而如今，theano都已经销声匿迹，只剩下了tensorflow和pytorch在争霸了。

深度学习的框架中，主要有两种分布式架构，分别是**Parameter Server-Workers**和**All-Reduce**。

Parameter Server-Workers有两部分，分别是Parameter Server和Wokers，前者是用来存储和处理参数的，后者是用来负责处理数据的，如果是多个Parameter Server和Woker的话，就分别只负责一部分。Parameter Server从Work中获取数据计算后的梯度信息，然后更新参数，Work在处理的时候会从Parameter Server上获取参数数据进行数据处理，得到当前的数据梯度信息。

All-Reduce的模式是每台机器既负责计算，也负责模型参数的更新。把数据分成n份，分别在每台机器上计算，然后求一个平均的梯度，然后把所有机器上的梯度都按照这个参数进行更新。

## 第三章 多层感知机在自然语言处理方面的应用
自然语言处理中最简单的任务就是计算两个句子或者两篇文档之间的相似度。

要解决这个问题，首先要把词和文本在数学上表示出来。可以通过**bag-of-words**进行建模。假设一篇文档中有100不同的词，那么我可以用一个1000维的向量来表示某一个词，这个词出现的索引置为1，其余地方置为0，所有的单词就都能用1000维的向量唯一表示出来了。那么一篇文档的向量就可以通过把这篇文档中的每一个词向量相加得到，也就是如果这篇文章中出现了这个词，那么对应的索引上就置为1，没有出现就是0。

但是如果只是看有没有出现，还是有点粗糙，于是发明了**TF-IDF**算法。向量中不仅表示文档中是否出现了该词，更是表示该词在文档中的重要程度。对于任意一个词，以这个词在当前文档出现的次数为分子，以这个词在所有的文档中出现的次数的$log$值为分母，分子处以分母的值就是这个词的权重。这个也很好理解，但是很妙，如果某个词只在当前文档出现，说明这个词比较与众不同，能够反应这个文档和其他文档的差别。像“的”这种单词在什么文档中都能出现，它的价值就很小，信息量很少。

上面的做法有个缺点就是**无法反应语义**，比如“称赞”和“夸奖”其实表达的是一个意思，但是在词向量中就是完全不同了。

因此后来发展出了**Word2Vec**算法。

目前最常见的Word2Vec算法有两种，分别是CBOW(Continuous Bag-of-Words)算法和Skip-Gram算法。他们的本质是一样的，而且互为镜像。

对于一个单独的字来说是没有语义的，语义是体现在上下文中的，什么词跟在什么词后面是具有一定模式的，这种模式也产出了语义。我们可以通过一个上下文窗口，把当前的词和其前后的一个或多个词进行关联，构建出正样本数据及其标注。负样本进行抽样学习。

CBOW算法就是以上下文为输入，当前单词为输出，通过上下文预测当前词，而Skip-Gram算法是以当前单词为输入，上下文为输出，通过当前单词预测其上下文。

把当前单词转化为上下文就需要一个编码映射，这个就是我们要学习的模型。可以通过前面的简单的方法把单词映射为特征向量。

但是Word2Vec有一些缺陷，比如：1.固定短语没法识别，比如"Best Buy"其实是一家电商，但是如果放到上下文就不能理解了，可以先识别短语，然后把短语当成一个整体。2.有些词词性完全不同，但是它的上下文一样，比如"It was a bad weather yesterday"和"It was a good weather yesterday"。3.一词多义的问题很难解决。4.当词在语料中出现的次数很少时就不能很好的学习到高维信息了，有些人提出了基于词根的建模，一定程度上可以缓解这个问题。

由于负样本的数量特别多，所以有一种方法是负样本取样，我之前一直理解的就是我直接选取一个正样本，然后选取几个负样本，然后把正样本和负样本合在一起计算softmax不就可以了嘛？但是好像这种做法有问题。照理说应该随机采样。书中参考论文《Distributed Representations of Words and Phrases and their Compositionality》得到了采用采样的方法计算的loss，但是原文我也没看出有这个公式的推导。

$$
J_k(\theta) = \log(1 + \exp^{(-z^1)}) + \sum_n\log(1 + \exp^{z^{i_n}})
$$

其中$i_n\in \{0,1,...,9\},i_n \ne 1$，$i_n=1$是我们的正样本。

虽然论文中没有推公式，但是我们理解起来没有那么困难。其实就是让正样本的$\exp^{(-z^1)}$越大，那么loss越小，而负样本的$\exp^{z^{i_n}}$值越大，那么造成loss越大，这样让正样本的概率越来越大。


## 第十三章 深度学习的下一个浪潮
我觉得未来的深度学习还是得在强化学习上面，其他方面也肯定大有作为，想想看互联网刚出来的时候不过是应用在很小范围内，但是现在几乎生活的方方面面都是计算机和互联网，未来深度学习或者机器学习会普及到生活的方方面面，显然现在还远远没有达到。