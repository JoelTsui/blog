---
title: 《深度学习模型及应用详解》读书笔记 
date: 
mathjax: true
---

## TODO
1. 什么是`nce_loss`，它`sampled_softmax_loss`之间的区别。
2. dropout的起源和使用。
3. Batch Normalization具体的计算和推导

## 第二章 深度学习开源框架
深度学习的变化真的很快，在2015年的时候，tensoflow刚出来，theano还是深度学习框架的霸主，而如今，theano都已经销声匿迹，只剩下了tensorflow和pytorch在争霸了。

深度学习的框架中，主要有两种分布式架构，分别是**Parameter Server-Workers**和**All-Reduce**。

Parameter Server-Workers有两部分，分别是Parameter Server和Wokers，前者是用来存储和处理参数的，后者是用来负责处理数据的，如果是多个Parameter Server和Woker的话，就分别只负责一部分。Parameter Server从Work中获取数据计算后的梯度信息，然后更新参数，Work在处理的时候会从Parameter Server上获取参数数据进行数据处理，得到当前的数据梯度信息。

All-Reduce的模式是每台机器既负责计算，也负责模型参数的更新。把数据分成n份，分别在每台机器上计算，然后求一个平均的梯度，然后把所有机器上的梯度都按照这个参数进行更新。

## 第三章 多层感知机在自然语言处理方面的应用
自然语言处理中最简单的任务就是计算两个句子或者两篇文档之间的相似度。

要解决这个问题，首先要把词和文本在数学上表示出来。可以通过**bag-of-words**进行建模。假设一篇文档中有100不同的词，那么我可以用一个1000维的向量来表示某一个词，这个词出现的索引置为1，其余地方置为0，所有的单词就都能用1000维的向量唯一表示出来了。那么一篇文档的向量就可以通过把这篇文档中的每一个词向量相加得到，也就是如果这篇文章中出现了这个词，那么对应的索引上就置为1，没有出现就是0。

但是如果只是看有没有出现，还是有点粗糙，于是发明了**TF-IDF**算法。向量中不仅表示文档中是否出现了该词，更是表示该词在文档中的重要程度。对于任意一个词，以这个词在当前文档出现的次数为分子，以这个词在所有的文档中出现的次数的$log$值为分母，分子处以分母的值就是这个词的权重。这个也很好理解，但是很妙，如果某个词只在当前文档出现，说明这个词比较与众不同，能够反应这个文档和其他文档的差别。像“的”这种单词在什么文档中都能出现，它的价值就很小，信息量很少。

上面的做法有个缺点就是**无法反应语义**，比如“称赞”和“夸奖”其实表达的是一个意思，但是在词向量中就是完全不同了。

因此后来发展出了**Word2Vec**算法。

目前最常见的Word2Vec算法有两种，分别是CBOW(Continuous Bag-of-Words)算法和Skip-Gram算法。他们的本质是一样的，而且互为镜像。

对于一个单独的字来说是没有语义的，语义是体现在上下文中的，什么词跟在什么词后面是具有一定模式的，这种模式也产出了语义。我们可以通过一个上下文窗口，把当前的词和其前后的一个或多个词进行关联，构建出正样本数据及其标注。负样本进行抽样学习。

CBOW算法就是以上下文为输入，当前单词为输出，通过上下文预测当前词，而Skip-Gram算法是以当前单词为输入，上下文为输出，通过当前单词预测其上下文。

把当前单词转化为上下文就需要一个编码映射，这个就是我们要学习的模型。可以通过前面的简单的方法把单词映射为特征向量。

但是Word2Vec有一些缺陷，比如：1.固定短语没法识别，比如"Best Buy"其实是一家电商，但是如果放到上下文就不能理解了，可以先识别短语，然后把短语当成一个整体。2.有些词词性完全不同，但是它的上下文一样，比如"It was a bad weather yesterday"和"It was a good weather yesterday"。3.一词多义的问题很难解决。4.当词在语料中出现的次数很少时就不能很好的学习到高维信息了，有些人提出了基于词根的建模，一定程度上可以缓解这个问题。

由于负样本的数量特别多，所以有一种方法是负样本取样，我之前一直理解的就是我直接选取一个正样本，然后选取几个负样本，然后把正样本和负样本合在一起计算softmax不就可以了嘛？但是好像这种做法有问题。照理说应该随机采样。书中参考论文《Distributed Representations of Words and Phrases and their Compositionality》得到了采用采样的方法计算的loss，但是原文我也没看出有这个公式的推导。

$$
J_k(\theta) = \log(1 + \exp^{(-z^1)}) + \sum_n\log(1 + \exp^{z^{i_n}})
$$

其中$i_n\in \{0,1,...,9\},i_n \ne 1$，$i_n=1$是我们的正样本。

虽然论文中没有推公式，但是我们理解起来没有那么困难。其实就是让正样本的$\exp^{(-z^1)}$越大，那么loss越小，而负样本的$\exp^{z^{i_n}}$值越大，那么造成loss越大，这样让正样本的概率越来越大。

[示例代码](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L205)中提到，这里用了`tf.nn.nce_loss`而没有用`tf.nn.sampled_softmax_loss`，这块还没有仔细看。


## 第四章 卷积神经网络在图像分类中的应用
文中提到，引入ReLU激活函数后，可以减少梯度消失的影响，但是会带来另一个问题，有可能存在很少被激活的“死节点”。这个原因应该是小于0的部分梯度直接变成0造成的，但是，说明这部分的参数就是没用的啊，不可能参数都利用起来吧，就算是用sigmoid或者tanh激活函数，应该也会存在这种问题，只不过那种表现是梯度很小。

局部响应归一化(Batch Normalization)和仿生学中临近的非常活跃的神经元之间存在竞争机制，其实Normalization的操作很多地方都有，只不过这里是在特征图之间进行。

dropout最开始提出来是为了能够融合多个模型？这个没有看过具体的论文不太清楚。那么dropout应该用在哪一层呢？最后？还是可以用在中间。


## 第五章 递归神经网络
LSTM算法在2003年就已经在语音识别领域大有作为了，但是深度学习在2010年以后才真正普及，这是什么道理？

RNN和普通的深度学习网络不一样的地方就在于它有“记忆”的功能，对于普通的神经网络而言，不同的输入之间是无关的，其他的输入信息不会作用到另一个输入中，而RNN就把这个关系加上了。不同的输入其参数是共享的，并不是不同的模型，这个思维有点像在《python深度学习》里面提到的，把句子正向反向都输入到网络中进行处理，因为处理句子的模型参数应该是一套而不是两套，所以正向句子和反向句子输入的是一个模型，然后在后面把这两个输出连接到一起，这个也是同理，所有的语音应该是用同一套模型参数去处理，只不过RNN可以把之前的信息也加进来。

双向递归神经网络(Bidirectional Recurrent Neural Network, BRNN)和长短期记忆模型(Long Short-Term Memory, LSTM)是RNN的两种变种。

BRNN是一个信息可以是正向传入，也可以反向传入，这和上上段的思维是一样的。

LSTM是解决普通的RNN不能有效建模长距离依存关系的问题。下面一张图可以说是非常清晰的解释了什么是LSTM，比书上讲的那么多门清晰的多的多了。

<p align=center><img src="http://q0qh4z3h0.bkt.clouddn.com/LSTM3-chain.png" alt="LSTM" title style/>


## 第十三章 深度学习的下一个浪潮
我觉得未来的深度学习还是得在强化学习上面，其他方面也肯定大有作为，想想看互联网刚出来的时候不过是应用在很小范围内，但是现在几乎生活的方方面面都是计算机和互联网，未来深度学习或者机器学习会普及到生活的方方面面，显然现在还远远没有达到。